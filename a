% -----------------------------------------------
% Template for ISMIR Papers
% 2025 version, based on previous ISMIR templates

% Requirements :
% * 6+n page length maximum
% * 10MB maximum file size
% * Copyright note must appear in the bottom left corner of first page
% * Clearer statement about citing own work in anonymized submission
% (see conference website for additional details)
% -----------------------------------------------

\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[submission]{ismir} % Remove the "submission" option for camera-ready version
\usepackage{amsmath,cite,url}
\usepackage{graphicx}
\usepackage{color}
\usepackage{mathabx}
\usepackage{subcaption}
\usepackage{booktabs}

\usepackage{enumitem}

\newcommand{\ziyu}[1]{\textcolor{blue}{#1}}
\newcommand{\heqi}[1]{\textcolor{green}{#1}}


% Title. Please use IEEE-compliant title case when specifying the title here,
% as it has implications for the copyright notice
% ------
\title{TOMI: Transforming and Organizing Music Ideas For Multi-Track Compositions with Full-Song Structure}

% Note: Please do NOT use \thanks or a \footnote in any of the author markup

% Single address
% To use with only one author or several with the same address
% ---------------
\oneauthor
  {Anonymous Authors}
  {Anonymous Affiliations\\\texttt{anonymous@ismir.net}}

% Two addresses
% --------------
%\twoauthors
%   {First author} {School \\ Department}
%   {Second author} {Company \\ Address}

% Three addresses
% --------------
% \threeauthors
%   {First Author} {Affiliation 1 \\ \texttt{author1@ismir.edu}}
%   {Second Author} {Affiliation 2 \\ \texttt{author2@ismir.edu}}
%   {Third Author} {Affiliation 3 \\ \texttt{author3@ismir.edu}}

% Four or more addresses
% OR alternative format for large number of co-authors
% ------------
% \multauthor
%   {First author$^1$ \hspace{1cm} Second author$^1$ \hspace{1cm} Third author$^2$}
%   {{\bf Fourth author$^3$ \hspace{1cm} Fifth author$^2$ \hspace{1cm} Sixth author$^1$}\\
%   $^1$ Department of Computer Science, University, Country\\
%   $^2$ International Laboratories, City, Country\\
%   $^3$ Company, Address\\
%   {\tt\small CorrespondenceAuthor@ismir.edu, PossibleOtherAuthor@ismir.edu}
%   }

% For the author list in the Creative Common license, please enter author names.
% Please abbreviate the first names of authors and add 'and' between the second to last and last authors.
\def\authorname{F. Author, S. Author, and T. Author}

% Optional: To use hyperref, uncomment the following.
% \usepackage[bookmarks=false,pdfauthor={\authorname},pdfsubject={\pdfsubject},hidelinks]{hyperref}
% Mind the bookmarks=false option; bookmarks are incompatible with ismir.sty.

\sloppy % please retain sloppy command for improved formatting

\begin{document}

\maketitle

\begin{abstract}
Hierarchical planning is a natural way to structure complexity in modeling long-range data.  Aside from temporal hierarchy commonly considered in generative models, this paper explores an alternative: \textit{concept hierarchy}, which involves generating musical materials, transforming them, and organizing them into a complete composition---similar to motif development. To this end, we introduce TOMI (\textbf{T}ransforming and \textbf{O}rganizing \textbf{M}usic \textbf{I}deas) and develop a TOMI-based model for multi-track electronic music generation with full-song structure. We formally define a structured representation of multi-track music that models a composition as a sparse, four-dimensional space defined by clips (short audio or MIDI segments), sections (temporal positions), tracks (instrument layers), and transformations (elaboration methods). We leverage large language models (LLMs) via in-context learning to generate and operate on this structure. Furthermore, we integrate TOMI with the REAPER digital audio workstation, enabling interactive human-AI co-creation. Experimental results demonstrate that our approach produces high-quality electronic music with strong structural coherence.\footnote{The demos are available at: \url{https://tomi-2025.github.io/}. We will open-source the code upon acceptance.}

% Studies on long-term music generation typically rely on a temporal hierarchy, where higher-level representations capture coarse context that guides lower-level synthesis. While effective for maintaining continuity, these time-only approaches overlook the compositional process in which musicians first develop basic ideas or motifs that are then elaborated into full compositions. To address this limitation, we propose the TOMI (Transforming and Organizing Music Ideas) paradigm for electronic music. Inspired by the prevalent use of MIDI and audio sample packs among electronic music producers, TOMI models a musical piece as a sparse, four-dimensional space defined by clips (short audio or MIDI segments), sections (temporal positions), tracks (instrument layers), and transformations (elaboration methods). We integrate a pre-trained large language model via in-context learning with this novel data structure to achieve multi-track full-song electronic music generation. Moreover, by integrating TOMI with the REAPER digital audio workstation, our system is able to provide an interactive workflow and exports high resolution audio. Experimental results show that our method generates high-quality electronic music with robust structural consistency. We show our demos on a demo page \footnote{\ziyu{Footnote after period. You should always write something here:} \url{https://tomi-2025.github.io/}}.
\end{abstract}

% ziyu:
% a 一个表示music idea（concept）的层级结构：这就是node对吧
% b 一个让层级idea和时空交互的方法 （时空说的就是 time & track）：这就是composition link对吧
% c 一个全自动借助LLM生成的方法：应该就是in context learning的部分？
% abc对应methodology三个subsection
% 1. intro的格局放大。从long-term music gen开始说，绝大多数是大力出奇迹，少数人认识到音乐有hierarchy，但也只局限在*时间层级*。在作曲的时候，更关键的是idea本身有层级关系（可以视为一种特殊的结构）。

% 作品本身可以看做是ideas在时空上的排列组合（可以放出daishu的文章佐证，还可以引述david cope当年的EMI就是这么干的，只是工具太老旧），但没有DL算法去model这一点。我们的TOMIC干的是这个事情。




\section{Introduction}

Automatic music generation has advanced from producing short clips to composing entire pieces, yet long-term structure remains a major challenge. Unlike short-term generation, which focuses on capturing local patterns \cite{scg,polydiff,riffusion,deepbach,musiclm,musicgen}, long-term generation requires handling structure across multiple levels---from sectional repetition and cadence to the overall theme and whole narrative flow. The most common approach is to scale up models and data \cite{jukebox,yue}, yet even with vast training corpora, achieving truly structured compositions remains difficult. An alternative is to model the music \textit{temporal hierarchy}, where musical evolution unfolds across multiple time scales, with different model components capturing context dependencies at specific levels \cite{wholesonggen,musicframeworks,musicvae,jukebox}.

% In long-term music generation, a majority of recent approaches train end-to-end models that output either symbolic or audio data \cite{yue,wholesonggen,jukebox}. These methods generally do not consider the music structure, and their performance is largely dependent on the quantity and quality of the training data. For some that consider music structure, they typically use a time-only hierarchy to maintain long-term structure \cite{wholesonggen,musicframeworks,musicvae,jukebox}. These studies are based on the idea that higher-level representations capture coarser and deeper context dependencies, which then guide lower-level synthesis. In such frameworks, a temporal planner sets the overall structure while lower-level models generate detailed musical content within these boundaries. This time-based decomposition has proven effective for ensuring continuity and coherence over long durations.

But have we fully captured the hierarchical nature of music? While temporal hierarchy is essential, so is \textit{concept hierarchy}, which often manifests in the development of motifs and musical materials. As noted in \cite{missingdeepmusic}, pop music is often built around a sparse set of core ideas, which evolve and repeat in an organic way. We specify this process in \figref{fig:fig1}, where musical ideas are first \textit{created}, then \textit{transformed}, and finally \textit{organized} into a full composition. We refer to this concept hierarchy in music as TOMI (\textbf{T}ransforming and \textbf{O}rganizing \textbf{M}usic \textbf{I}deas). TOMI shares a spirit with David Cope’s recombinant music composition in EMI \cite{emirecombinant}, an idea that deep learning has yet to truly embrace.

% However, this time-only approach does not fully capture the creation process of composition. Composers do not simply predict music elements at different levels; they often start by developing basic music ideas or motifs that are then repeated and expanded. As demonstrated in MusicFrameworks \cite{musicframeworks}, pop music typically uses a sparse set of recurring themes that are later elaborated into detailed time series. Thus, a composition can be seen as a permutation of music ideas in space-time. This perspective is also echoed in EMI Recombinant Music \cite{emirecombinant}, but it's now obsolete due to recent advances in deep learning.
% In our work, we refer to this alternative process as TOMI (Transforming and Organizing Music Ideas).
% TOMI emphasizes first creating high-level music elements, such as phrases or motifs, and subsequently expanding these ideas into a full composition structure.

\begin{figure}
    \centering
    \includegraphics[alt={TOMI structure},width=0.9\linewidth]{images/tomi_structure.jpg}
    \caption{\heqi{Concept hierarchy in TOMI: a composition is a space-time permutation of transformed musical ideas across tracks and sections, with multiple clips can share the same transformation.}}
    % \caption{The left plot shows an example of three composition links and how they can be projected onto a traditional DAW arrangement view; the right plot demonstrates our approach for music data representation and how multiple composition links can form a music piece with node reuse. The arrows with the same color belongs to the same composition link, and the state mappings for the state sequence of transformation nodes are: \textit{start} ($\blacktriangleright$), \textit{continue} (=) and \textit{pause} (-).}
    \label{fig:fig1}
\end{figure}

In this paper, we propose a TOMI-based music generation system for full-song and multi-track electronic music composition, inspired by the prevalent use of MIDI and audio sample packs among electronic music producers. The system is built around four key elements: \textit{clips}, \textit{transformations}, \textit{sections}, and \textit{tracks}. Sections and tracks are first created to serve as the canvas for composition, similar to a digital audio workstation (DAW) interface. The system then selects musical materials from a library of audio and MIDI \heqi{samples}. For each selected material, a transformation function is defined and applied, then the transformed clip is placed in its designated section and track. On the backend, a structured data representation parameterizes clips, transformations, sections, and tracks, and links them dynamically to construct a full composition. We leverage a pre-trained text-based large language model (LLM) to operate on this data structure, using in-context learning \heqi{(ICL)} to fill in the parameters and create dynamic links.

In sum, the contributions of this paper are as follows:
\setlength{\leftmargini}{\baselineskip}
\begin{enumerate}[noitemsep]
    \item \textbf{We introduce TOMI to model music concept hierarchy} and develop a deep learning-based system for structured electronic music generation. The proposed data structure integrates symbolic and audio representations and is operable by \heqi{LLMs via ICL.}
    \item \textbf{We apply TOMI to generate high-quality electronic music with full-song structure}. Objective and subjective measurements show that songs generated by our model have more identifiable phrase boundaries, better-structured phrase development, and higher music quality compared to baselines.
    \item \textbf{We integrate TOMI with the REAPER digital audio workstation}, enabling interactive human-AI co-creation and high\ziyu{-}resolution audio rendering.
\end{enumerate}


% We introduce the TOMI data structure to model the whole piece as a structured space defined by four types of elements: (1) \textit{clips}, representing short audio or MIDI segments; (2) \textit{sections}, indicating positions along the time axis; (3) \textit{tracks}, indicating positions along the instrument axis; and (4) \textit{transformations}, defining the elaboration methods. We consider electronic music a good starting point, especially for TOMI, because electronic music production commonly involves various transformations (e.g., repetition and slicing) applied to music ideas. In addition, electronic music composers naturally use sample data during composing, and therefore we can leverage existing music samples to facilitate our generation.}


% We use a pre-trained large language model (LLM) with TOMI data structure and achieve full-song electronic music generation through in-context-learning (ICL) and sample retrieval. We also integrate TOMI with the REAPER digital audio workstation to enable interactive co-creation and high-resolution audio rendering. Our experiments prove that our system is capable of generating high quality electronic music with robust structural consistency.

% In this paper, we propose the TOMI (Transforming and Organizing Music Ideas) paradigm referring to this composition process and apply it to multi-track full-song generation in electronic music style. Our idea comes from the growing popularity of music sample packs among electronic music composers. When using sample data, composers select a suitable MIDI or audio sample, apply transformations (e.g., repeating or slicing), and assign it to a specific track and section. Next, we use a pre-trained large language model (LLM) with TOMI and achieve full-song electronic music generation through in-context-learning (ICL). We also integrate TOMI with the REAPER digital audio workstation to enable interactive co-creation and high-resolution audio rendering. Our experiments prove that our system is capable of generating high quality electronic music with robust structural consistency.

\section{Related Work}
There is a wide range of models existing for music generation. However, most of them can only generate a short piece of music without a clear song structure \cite{scg,polydiff,riffusion,deepbach,musiclm,musicgen}. For methods that consider structures, Jukebox \cite{jukebox} uses hierarchical VQ‑VAE with time conditioning to facilitate long-term structure; Whole-Song Hierarchical Generation \cite{wholesonggen} applies cascaded diffusion models for structured symbolic music generation. Some methods introduce explicit structure encoding in neural networks \cite{structureencodding} or use efficient sampling techniques to generate structured variations \cite{samplingvariations}. However, these approaches focus solely on temporal structuring. There are few previous studies adopting the TOMI concepts in full song generation; the most related works include a rule-based recombinant music method \cite{emirecombinant}, which reorganizes existing musical elements based on stylistic rules, and MELONS \cite{melons}, which uses a structure graph to enforce long-term dependencies in melody generation.

Existing generative music models typically focus on a single modality, which is either symbolic \cite{wholesonggen,melons} or audio form \cite{musicgen}. Moreover, most approaches lack interactive capabilities. Composer's Assistant \cite{composersassistant} and its successor \cite{composersassistant2} integrate their methods into REAPER to support co-creation, but they can only generate symbolic phrases rather than full pieces. Our approach not only generates complete compositions with both MIDI and audio phrases but also enables user co-creation in REAPER.

Recent work shows that large language models (LLM) have potential for music generation but also have some limits. ChatMusician \cite{chatmusician} and MuPT \cite{mupt} demonstrate that using LLM to directly generate music data can produce promising results, but the diversity of music is limited by their training data. Other models such as Mu‑Llama \cite{mullama} and Audio Flamingo \cite{audioflamingo} show strong music analysis abilities of LLM. Moreover, current LLMs have shown the ability to generate well-structured code and data \cite{gpt4o}. Inspired by these works, we leverage pre-trained LLMs to improve both music generation and music structure.

\begin{figure*}
    \centering
    \includegraphics[alt={TOMI process},width=0.9\linewidth]{images/fig2.jpg}
    \caption{\heqi{The LLM generation steps in \textit{i.} align with our prompt structure, and the corresponding TOMI process is shown in \textit{ii.}. For clip nodes, the LLM only generates their attributes, which then will be used as keywords to retrieval samples from databases. We also show the node reuse feature that section node $s_2$ is repeated in the section sequence, which means the two $s_2$ are identical and sharing the same \textit{composition links}.}}
    \label{fig:fig2}
\end{figure*}

\section{Methodology}
\ziyu{In this section, we discuss TOMI in multi-track electronic music generation with full-song structure. The implementation consists of two main components: (1) a graph data structure named \textit{composition link} that connects raw music ideas with the composition (Section~\ref{subsec:3:datastructure}), and (2) in-context learning to compose music by following this data structure (Section~\ref{subsec:3:icl}). We also demonstrate the integration with the REAPER DAW (Section~\ref{subsec:3:reaper}).}

% \ziyu{[\textit{Ziyu's note 2: for now, I think 3 subsections is better than 2. But section 3.3 may be organized as a standalone, very short section 4 as well. I cannot tell now.}]}

% We achieve full-song electronic music generation through ICL and using the TOMI data structure. The following sections detail our approach: Section 3.1 introduces the TOMI data structure, and Section 3.2 discusses the generation process of electronic music through ICL and integration with the REAPER digital audio workstation.


\subsection{TOMI Data Structure}
\label{subsec:3:datastructure}
We propose the TOMI data structure reflecting the creation process of a full piece: from elaborating music elements to organizing them into the final composition. The concepts of \textit{clips}, \textit{sections}, \textit{tracks}, and \textit{transformations} are represented as nodes, and the creation steps are represented as \textit{composition links}. A \textit{composition link} is a quadruplet of these nodes, showing a music clip (\textit{what}) to be placed in a particular section (\textit{when}) and on a specific track (\textit{where}), undergoing certain transformations (\textit{how}). In addition, the nodes are reusable among the composition links, and the full set of links forms the complete piece.

\subsubsection{Clip Node}
The basic musical materials are represented by clip nodes, each can be a chord progression, a melody, a drum-loop, etc. It is classified into two categories: MIDI and audio, each comprising different attribute sets aligned with its respective sample databases. MIDI clip attributes consist of \textit{content type} (chord, bass, melody, or arpeggio), two temporal properties (\textit{duration} in bars and \textit{tempo}), and five musical features (\textit{note sequences}, \textit{tonality}, \textit{genre}, \textit{time signature}, and \textit{numeric chord progression}). For audio clips, in addition to the same temporal properties and tonality features, the attributes also include a \textit{sample type} (distinguishing between loop-able and one-shot samples) and \textit{labels}. These labels comprise keywords categorizing the clip by genre, mood, instrument, and compositional role (e.g., sound effect, drum, melody). Once a clip node is created, its attributes would be used as queries to retrieve sample data from the databases.

\subsubsection{Section Node}
A music composition can be temporally divided into sections such as verse and chorus. We formalize this sequential structure through section nodes, which have two attributes: \textit{section length} (duration in bars) and \textit{phrase label} (categorized as intro, verse, pre-chorus, chorus, bridge, or outro). Sections with the same phrase label typically share similar or identical musical content and patterns, while different labels show contrast and progression. The start time of each section is computed dynamically according to the \textit{section sequence}, which determines the order of sections. We can handle repeated sections leveraging the node reuse feature. For implementation simplicity, we constrain all sections to the $4/4$ time signature, which is predominant in electronic music.

\subsubsection{Track Node}
The track nodes represent the space dimension of a composition that stacks clips vertically, with MIDI tracks requiring virtual instruments to produce sound and audio tracks playing audio clips directly. Each track node is defined by a single attribute: \textit{track type} (MIDI or Audio).
% and track role (its musical role within the composition, categorized as ambience, chord, bass, melody, arpeggio, main drum, percussion, sound effect, or vocal).
% For MIDI tracks, users have the flexibility to assign specific virtual instruments within the REAPER DAW, allowing for customized sound selection and manipulation.

\subsubsection{Transformation Node}
Musical clips require elaborate arrangement and positioning to create effective patterns. As shown in \figref{fig:fig2}, the transformation nodes function as intermediary processing layers before the clips are integrated into the composition. For example, a one-shot drum sample clip can be placed on a track in various ways with different transformations, such as on every beat or only the second and fourth beats of each bar to form a rhythmic pattern. There are three types of transformation nodes: (1) \textit{General Clip}, (2) \textit{Drum}, and (3) \textit{Fx (Sound Effect)}. For \textit{General Clip} and \textit{Drum}, the node is given an \textit{action sequence} attribute containing $n$ states classified as \textit{start}, \textit{sustain} and \textit{rest}. Each state corresponds to an action at a step time within the section (eg. a bar has 16 steps in $4/4$ time signature). The \textit{start} state means the clip will be replayed at this time, \textit{rest} means the clip will stop playing, and \textit{sustain} means to continue playing. Transformation nodes of \textit{General Clip} type will loop the clip if its duration is shorter than the interval between the \textit{start} state and either the first \textit{rest} state that follows or the end of the sequence; the \textit{Drum} transformation is designed for one-shot drum samples and will only play the clip once for each \textit{start} state. In addition, the length of the \textit{action sequence} is dynamically adjusted to match various durations of its associated sections, either by cropping from the end if the sequence is longer, or looping it otherwise. Lastly, the \textit{Fx} type transformation is designed for common riser and faller sound effects, hence it requires a \textit{placement} attribute instead of \textit{action sequence} to specify whether the clip attaches to the section's start or end.

\subsubsection{Composition Link}
A composition link comprises a section node, a clip node, a transformation node, and a track node. To avoid data redundancy and enforce consistency for repeated elements, the nodes are defined independently of the composition link, allowing them to be shared among multiple links.
For example, a clip can be referenced by 5 composition links associated with a total of 3 sections, 3 transformations, and 2 tracks. This means that the same clip appears in all these sections, although it may take different forms and appear on different tracks.
With node reuse and disentanglement of music concepts, the composition data can therefore be preserved more intuitively and efficiently. Furthermore, this data structure can encapsulate complex music arrangements in a high-level yet simple representation, which also provides advantages for LLM-based generation.

\subsection{Music Generation with In-Context Learning}
\label{subsec:3:icl}

Our system leverages the contextual understanding and structured generation capabilities of text-based LLMs to generate full-song electronic music. We carefully design the prompt to specify generation requirements and instructions for the LLM through in-context learning (ICL).

% \subsubsection{System Prompt Structure}
In our prompt structure, we elaborate the TOMI data structure, explain each node type along with its attributes, and define the schema for structured output. \heqi{A prompt example is shown on our demo page.} It sets the context for the generation through the following steps:
\setlength{\leftmargini}{\baselineskip}
\begin{enumerate}[noitemsep]
    \item The role “professional music composer” is assigned to the LLM and the TOMI data structure is introduced.
    \item The four node types are elaborated with examples, and the response schema for creating valid nodes is specified as a sequence containing a unique node name along with its attributes.
    \item Composition link is elaborated, then the model is instructed to use the names of generated nodes in links to avoid non-existent ones. The feature of node reuse is emphasized to avoid redundant generations.
    \item The output of the complete composition is structured in a specified key-value format, with keys following the order: \textit{sections} (with section sequence), \textit{clips}, \textit{transformations}, \textit{tracks}, and \textit{composition links}. As shown in \figref{fig:fig2}, this schema guides the LLM to create a palette of nodes first, then the nodes would become context for creating composition links, thereby improving the stability and consistency of the generation.
    \item The LLM is prompted to generate a full composition using the data structure, with optional contexts, such as tempo, mood, and instrument preferences. Furthermore, we can also specify custom section sequences here for the LLM to use.
\end{enumerate}

% \textit{User Prompt: Please compose an instrumental electronic music. Feel free to choose any instruments you like. The tempo is about 120 BPM, and the mood is happy.}

\subsubsection{Generation Process}
We initiate the generation process by sending the prompt to the LLM.
To have robust generations, we implement a rule-based validation to check LLM output for syntax errors, incorrect values, and invalid node names. When these issues are found, it will generate an error report and prompt the LLM to refine its response iteratively until no errors are detected. Moreover, we develop a data processing pipeline to convert the generated text-based data to instances. By parsing the composition links, the nodes are interconnected based on their assigned names. Then, the clip nodes retrieve samples from the databases based on their given attributes. If there is no matching sample for a clip node, the node will be skipped along with its associated links in the final arrangement.

% \subsubsection{Digital Audio Workstation Integration}

\subsection{Digital Audio Workstation Integration}
\label{subsec:3:reaper}
To support audio rendering and interactivity, we integrate the TOMI framework with the REAPER DAW. This allows the generated composition to be visualized in a professional DAW while benefiting users from REAPER’s editing and rendering capabilities. The composition links and nodes of a generated piece are converted to REAPER elements, with tracks, section markers, and clips applied with their transformations. This process relies on two user defined global settings: \textit{tempo} and \textit{tonality}. The system automatically time-stretches loop-able clips to fit the \textit{tempo} and transposes melodic clips to align with the \textit{tonality}.

\section{Experiment}
We implement a TOMI-based generation system for electronic music using GPT-4o \cite{gpt4o} and prepare two databases for sample retrieval. Next, we conduct experiments comparing our approach with three methods. In this section, we discuss how we prepare our databases (Section~\ref{subsec:4:database}) and baseline settings (Section~\ref{subsec:4:baseline}), then evaluate the music quality and structural consistency based on two objective metrics and four subjective assessments (Section~\ref{subsec:4:eval}).
\subsection{Clip Databases and Sample Retrieval}
\label{subsec:4:database}
We collect multiple licensed MIDI packs and audio sample packs in electronic music genre through online purchases.\footnote{\url{https://splice.com/}, \url{https://www.loopmasters.com/}} Then we process the raw datasets into a MIDI database and an audio database using different feature extraction methods.
For MIDI samples, we develop a script to analyze and extract musical features (same as the MIDI clip node's attributes) from a MIDI file. Moreover, it can also extract musical stems, such as bass, chord, and melody, from the source MIDI to augment the data. Then we store the labeled data in a SQLite3 \cite{sqlite} database as our MIDI database. For audio samples, we use ADSR Sample Manager \cite{adsr} to analyze and generate labels for our audio dataset. The results are also exported as a SQLite3 database. The details of our MIDI database and audio database are shown in \tabref{tab:databases}.

The sample retrieval mechanism for clip nodes is achieved by constructing a search query based on the clip's attributes then retrieving from its respective database. The clip node randomly selects one sample to use from all matching results; if no matching sample is found, the clip along with its associated composition links is discarded.
\begin{table}
    \centering
    \small
    \subfloat[MIDI content types.]{
        \begin{tabular}{lr}
            \hline
            Content Type & Count \\
            \hline
            Chord & 2604        \\
            Bass & 209        \\
            Melody & 1392        \\
            Arpeggio & 227        \\
            \textbf{Total} & \textbf{4432} \\
            \hline
        \end{tabular}
    } \quad
    \subfloat[MIDI durations.]{
        \begin{tabular}{lr}
            \hline
            Duration & Count \\
            \hline
            4-bar & 2947        \\
            8-bar & 1417        \\
            16-bar & 68        \\
            & \\
            \textbf{Total} & \textbf{4432} \\
            \hline
        \end{tabular}
    }
    \vspace{\baselineskip}
    \subfloat[Audio sample types.]{
        \begin{tabular}{lr}
            \hline
            Sample Type & Count \\
            \hline
            Loop & 104493        \\
            One-Shot & 170187        \\
            & \\
            & \\
            \textbf{Total} & \textbf{274680} \\
            \hline
        \end{tabular}
    } \quad
    \subfloat[Audio loop durations.]{
        \begin{tabular}{lr}
            \hline
            Loop Duration & Count \\
            \hline
            2-bar & 24922        \\
            4-bar & 27214        \\
            8-bar & 24638        \\
            16-bar & 27719        \\
            \textbf{Total} & \textbf{104493} \\
            \hline
        \end{tabular}
    }
    \caption{Statistics of sample databases.}
    \label{tab:databases}
\end{table}
\subsection{Baseline Method and Ablations}
\label{subsec:4:baseline}
We compare TOMI with MusicGen \cite{musicgen} and two ablations in electronic music generation. For exporting audio via REAPER, there are 8 virtual instrument presets (5 for chord, 2 for melody, and 1 for bass) for MIDI tracks to choose from randomly. We keep all REAPER settings to default, and no mixing plug-ins are applied except for a limiter on the master track to prevent audio clipping.

\begin{table*}
    \centering
    \small
    \begin{tabular}{lrrrrr}
        \toprule
        Method & $\text{FAD}^{VGGish} \downarrow$ & $\text{FAD}^{CLAP} \downarrow$ & $\text{ILS}^{MERT} \uparrow$ & $\text{ILS}^{MS} \uparrow$ & $\text{ILS}^{WF} \uparrow$ \\
        \midrule
        TOMI & \textbf{3.51} & \textbf{0.38} & \textbf{0.28} $\pm$ 0.12 & \textbf{0.36} $\pm$ 0.33 & \textbf{1.14} $\pm$ 0.73 \\
        MusicGen & 5.31 & 0.62 & 0.06 $\pm$ 0.04 & 0.12 $\pm$ 0.07 & 0.28 $\pm$ 0.09 \\
        Standalone LLM & 5.84 & 0.46 & 0.16 $\pm$ 0.11 & 0.10 $\pm$ 0.12 & 0.09 $\pm$ 0.16 \\
        Random & 6.92 & 0.47 & 0.16 $\pm$ 0.09 & 0.22 $\pm$ 0.16 & 0.48 $\pm$ 0.28 \\
        \bottomrule
    \end{tabular}
    \caption{Objective evaluation results of FAD with two models and ILS with three latent representations.}
    \label{tab:objective_eval}
\end{table*}
\begin{figure*}
    \centering
    \includegraphics[alt={ILS example image},width=0.9\linewidth]{images/figure2.png}
    \caption{ILS self-similarity matrices example, where all four compositions are generated under the section sequence: \textit{i} (intro), \textit{v1} (verse 1), \textit{p1} (pre-chorus 1), \textit{c1} (chorus 1), \textit{v2} (verse 2), \textit{p2} (pre-chorus 2), \textit{c2} (chorus 2), \textit{b} (bridge), \textit{c3} (chorus 3), and \textit{o} (outro). The blocks marked as yellow-edge rectangles are same-label similarities, with diagonal values marked as red lines being excluded, and the remaining parts are different-label similarities.}
    \label{fig:ils}
\end{figure*}
\subsubsection{MusicGen}
We use MusicGen-Large-3.3B model as the baseline, with prompts that specify tonality, tempo, and section sequence. It helps to evaluate our approach against state-of-the-art music generation systems. Given MusicGen's 30-second generation limit, we implement a sliding window approach to generate longer audio by using a fixed 30-second window that slides in 10-second chunks, while using the previous generated 20 seconds as context. To enable structural awareness during generation, we modify the model's inference process by adding explicit structure context after the initial text prompt at each generation step, instructing the model to align its output with the given structure. \heqi{A prompt example is shown on our demo page.}

% (1) \textit{User Prompt: Generate an electronic track at 120 BPM in C major. Follow this structure: 8-bar intro, 16-bar verse…}
% (2) \textit{Structure Context: For now, you are generating the segment between the 120th second and the 150th second, which corresponds to the the 60th bar and the 75th bar, your generated segment includes 4 bars of PreChorus, 8 bars of Chorus, and 3 bars of Outro.}

\subsubsection{Standalone LLM (TOMI w/o Composition Links)}
This ablation uses the same GPT-4o model but without using composition links. It helps to evaluate the impacts of composition links representation on generation quality. We design the prompt to let the LLM generate a sequence of tracks and clip descriptions with position information (time point and track location) conditioned on a section sequence. The sample retrieval mechanism is also applied for clips.

\subsubsection{Random (TOMI w/o LLM)}
To assess the contribution of LLM-based generation, we implement a rule-based ablation that uses the same TOMI framework but generates arrangements through a series of randomized operations. It first creates a random number (15-25) of track nodes, then populates sections with clips based on stochastic decisions. For each track on each unique section, the system determines clip placement through a two-step random process: first deciding whether to place a clip, if so, then choosing between reusing an existing clip or generating a new one. For each MIDI clip, the content type is randomly assigned one of three types (chord, bass, or melody), with bass clips comprise root notes derived from existing chord clips to maintain coherence. The generation of audio clips involves random selection from a predefined set of feature labels, which cover tonal elements, percussion, and sound effects. The final step links one of four predefined transformations to each clip based on the clip's content type or audio features.
\begin{figure*}
    \centering
    \includegraphics[alt={subjective eval result image},width=0.7\linewidth]{images/figure3.png}
    \caption{Subjective evaluation results of mean score with standard error or mean (SEM), where p1-p4 corresponds to \textit{local music quality}, \textit{consistency among same-label phrases}, \textit{contrast between different-label phrases}, and \textit{overall full-song evaluation} respectively.}
    \label{fig:subjective_eval}
\end{figure*}
\subsection{Evaluation}
\label{subsec:4:eval}
\subsubsection{Objective Evaluation}
We use the Fréchet Audio Distance metric (FAD) \cite{fad} to evaluate the music quality of generated audios and propose a refined version of the Inter-Phrase Latent Similarity metric (ILS) \cite{wholesonggen} to measure the composition’s structural consistency. We predefine 4 tonalities
% (C major / A minor, F major / D minor, G major / E minor and B$\flat$ major / G minor)
and 4 distinct section sequences, each comprising a sequence of section names with phrase labels and durations. Sections with the same name and phrase label should be identical, and those with different names but the same label should be similar. Then, we generate four sets of electronic music pieces of 120 BPM using each method, with each set containing 8 compositions generated under the same section sequence and 4 different tonalities (2 compositions for each key).

We use a PyTorch-based FAD implementation \cite{fad_pytorch} with a VGGish model \cite{vggish} and a CLAP model \cite{clap} to compare human-composed electronic music and the music generated by each method. We randomly collected 329 songs as references from the Spotify Mint playlist \cite{spotify}, one of the most popular curated playlists dedicated to the electronic music genre. A lower FAD score indicates the generated music is closer to human-composed music in quality.

The ILS score is measured based on the self-similarity matrix of the composition's latent representations. We redesign the ILS metric to improve its effectiveness. We first exclude diagonal elements to avoid self-similarity bias. Instead of using the ratio of same-label and overall similarities in the original implementation, we use Cohen's \textit{d} \cite{cohensd} to measure the difference between same-label and different-label similarities. A higher ILS score indicates better and more effective structural consistency. The formula of our ILS implementation is as follows:
\begin{equation}
\text{ILS} = \frac{\bar{X}_\mathrm{same} - \bar{X}_\mathrm{diff}}{s}
\end{equation}
with
\begin{equation}
\bar{X}_\mathrm{same} = \frac{\sum_{i\neq j} S_{ij} \cdot \delta(l_i = l_j)}{N_\mathrm{same}}
\end{equation}
\begin{equation}
\bar{X}_\mathrm{diff} = \frac{\sum_{i, j} S_{ij} \cdot \delta(l_i \neq l_j)}{N_\mathrm{diff}}
\end{equation}
\begin{equation}
s = \sqrt{\frac{(N_\mathrm{same} - 1)s_\mathrm{same}^2 + (N_\mathrm{diff} - 1)s_\mathrm{diff}^2}{N_\mathrm{same} + N_\mathrm{diff} - 2}}
\end{equation}
where $S_{ij}$ represents the cosine similarity between the latent elements $i$ and $j$ in the self-similarity matrix, and $l_i$ denotes the phrase label of element $i$. The pooled standard deviation $s$ is derived from the standard deviations of the two similarity groups $s_\mathrm{same}^2$ and $s_\mathrm{diff}^2$. $N_\mathrm{same}$ and $N_\mathrm{diff}$ are the sizes of same-label and different-label elements respectively, with diagonal elements excluded. We evaluate multiple audio representation methods, including MERT embeddings \cite{mert}, MelSpectrograms, and raw waveforms, denote as $\text{ILS}^{MERT}$, $\text{ILS}^{MS}$, and $\text{ILS}^{WF}$, respectively.

We show the FAD scores and ILS with mean and standard deviations in \tabref{tab:objective_eval}, and we show the example self-similarity matrices of compositions in \figref{fig:ils}. The results demonstrate that our method surpasses the baseline and ablations in both music quality and structural consistency, proving its ability to create music with better flow naturalness and arrangement coherency, while maintaining long-term music structure align with phrase labels.

\subsubsection{Subjective Evaluation}
We conduct a double-blind online survey and prepare three sets of questions with the same evaluation structure but evaluating different compositions. Participants are randomly assigned one question set upon entering the survey. For each set, participants compare and evaluate four compositions (TOMI, MusicGen, and 2 ablations) generated under the same conditions of key, tempo, and section sequence. All metrics are based on a 5-point rating scale, and we provide explanations for each metric to help participants understand.

The evaluation is based on four parts, each part consists of 1-3 pages and each page contains four questions regard to the audio pieces from each composition:

\textbf{local music quality.} This part consists of 3 pages. For each page, we pick the same section (eg. intro) from each composition. Participants rate \textit{Naturalness} to evaluate the similarity to human-composed music and the conformity to the typical electronic music style. The rating scale ranges from \textit{very low} to \textit{very high}.

\textbf{consistency among same-label phrases.} This part consists of 2 pages. For each page, we pick two sections with the same phrase label (eg. verse1 and verse2) from each composition. Participants rate \textit{Similarity} between the two sections. The rating scale ranges as: (1) \textit{almost irrelevant}, (2) \textit{very different (but somehow consistent)}, (3) \textit{similar (with noticeable differences)}, (4) \textit{very similar (with minor differences)}, and (5) \textit{almost the same}.

\textbf{contrast between different-label phrases.} This part consists of 2 pages. For each page, we pick two continuous sections from the same position of each composition (eg. intro and verse1). Participants rate \textit{Transition Naturalness} based on boundary clearness, transition smoothness, and whether the sections serve their intended roles. The rating scale ranges from \textit{very low} to \textit{ very high} and the explanations for each point are: (1) \textit{very low}: the boundary is unclear, the transition feels jarring or unnatural, and the sections do not complement or serve their intended roles; (2) \textit{low}: the song structure is weak but the boundary is somewhat discernible, and the transition lacks musicality and disrupts the flow; (3) \textit{neutral}: the boundary is noticeable, and the transition is passable but lacks fluidity, the sections serve their roles but could be more musically cohesive; (4) \textit{high}: the boundary is clear, with a smooth and musical transition, both sections fulfill their phrase structure effectively; and (5) \textit{very high}: the boundary is well-defined, with a seamless and natural transition, each section distinctly serves its role while enhancing the overall musicality and phrase structure.

\textbf{overall full-song evaluation.} This part consists of 1 page. We show the complete result of each generated composition. participants rate four criteria: \textit{Structure Clarity} for how well each section aligns with the given structure, \textit{Creativity} for how creative the music is, \textit{Naturalness} for how likely a human composer produces the music, and \textit{Musicality} for the overall musical quality. The rating scale of each criterion ranges from \textit{very low} to \textit{ very high}. We also provide the section sequence with time intervals on the page to allow participants skip through different parts of the composition.

For each generation method, we pick the audio segments from the same composition across different parts of the survey, this helps participants build up familiarity with the music and lead to more reliable evaluation results. We also insert an intermediate page between evaluation parts to inform participants of their progress and alleviate listening fatigue. Furthermore, we provide the full section sequence with tonality information as context on each question page to remind participants of the generation conditions.

We distributed the survey on multiple social media platforms, and we received a total of 73 responses.
% the demographic statistics of participants are as follows:
% \textit{Age} ($<$18: 0\%, 18-29: 69.86\%, 30-44: 19.18\%, 45-59: 5.48\%, $\geq$60: 5.48\%);
% \textit{Gender} (Female: 30.14\%, Male: 65.75\%, Non-binary: 2.74\%, Prefer not to say: 1.37\%);
% \textit{Musical background} (Amateur: 34.25\%, Intermediate: 41.1\%, Professional: 24.66\%);
% \textit{Years spent on studying music} (None: 26.03\%, 1 year: 6.85\%, 2 years: 10.96\%, 3-5 years: 13.7\%, 6-10 years: 16.44\%, $>$10 years: 26.03\%).
The metric results are shown in \figref{fig:subjective_eval}, where the bar height represents the mean score, and the error bar represents the standard error of mean (SEM) computed by within-subject ANOVA. The results show that TOMI significantly outperforms the baseline in most subjective metrics, proving the effectiveness of our system in generating high-quality electronic music with solid long-term structural consistency.
\section{Limitations and Future Work}
While our approach can generate electronic music of reasonable quality, the harmonic coherence is sometimes affected by randomness and limited features during sample retrieval. Our system currently selects samples solely from local sample collections using a small set of features, which can lead to empty search results or highly divergent samples sharing the same features. To address these issues, we plan to use generative models specifically for clip samples, or integrate ML-based MIDI and audio embedding models for better and more accurate materials. Moreover, to support sophisticated sound design and mixing capabilities in real-world music production, we intend to extend our current model with additional structural hierarchy and node types. Lastly, we plan to train a native TOMI-based neural network model on real-world music project files for higher-quality and more scalable generation.
\section{Conclusion}
In conclusion, we contribute TOMI, an intuitive paradigm for music data representation, and combine it with an ICL approach to achieve the first system for generating long-term, multi-track electronic music with both MIDI and audio clips. The experiment results prove that our approach achieves high quality generation with robust structural consistency. In addition, we integrate it with REAPER to support audio rendering and further co-creation.


\bibliography{ISMIRtemplate}
\end{document}
